{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Neural Networks Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will implement the feedforward algorithm for neural networks and apply it to the task of `Fashion MNIST`\n",
    "recognition.\n",
    "\n",
    "The same multi-class classification problem solved in the previous lab will be solved using Neural Networks. The same dataset will be used.\n",
    "\n",
    "Before we begin with the exercises, we need to import all libraries required for this programming exercise. Throughout the notebook, we will be using [`numpy`](http://www.numpy.org/) for all arrays and matrix operations, [`matplotlib`](https://matplotlib.org/) for plotting, and [`scipy`](https://docs.scipy.org/doc/scipy/reference/) for scientific and numerical computation functions and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to capture more complex hypothesis, addition of non-linear terms in Logistic Regression based Classification quickly falls short with data containing higher number of features, since the number of terms in the hypothesis will grow asymptotically with an order of **O(n<sup>2</sup>)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(X, example_width=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Displays 2D data stored in X in a nice grid.\n",
    "    \"\"\"\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "In this exercise, you will implement the feedforward propagation for neural networks to predict handwritten digits with the optimized weights already provided. Later, the backpropagation algorithm will also be implemented to learn the parameters for the neural network.\n",
    "\n",
    "We start the exercise by first loading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training data stored in arrays X, y\n",
    "data = pd.read_csv('fashion-mnist_train.csv')\n",
    "\n",
    "X,y = data.iloc[:, 1:], data.iloc[:, 0].ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualizing the data\n",
    "\n",
    "There are 10000 training examples in `fashion-mnist_train`, where each training example is a 28 pixel by 28 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 28 by 28 grid of pixels is “unrolled” into a 784-dimensional vector. Each\n",
    "of these training examples becomes a single row in our data matrix $X$. This gives us a 10000 by 784 matrix $X$ where every row is a training example for a handwritten digit image.\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\left(x^{(1)} \\right)^T - \\\\\n",
    "- \\left(x^{(2)} \\right)^T - \\\\\n",
    "\\vdots \\\\\n",
    "- \\left(x^{(m)} \\right)^T - \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The second part of the training set is a 10000-dimensional vector `y` that contains labels for the training set. \n",
    "The following cell randomly selects 100 images from the dataset and plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 100 data points to display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Our neural network is shown in the following figure.\n",
    "\n",
    "![](Figures/neural_network.png)\n",
    "\n",
    "It has 3 layers - an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values\n",
    "of digit images. Since the images are of size $28 \\times 28$, this gives us 784 nput layer units (not counting the extra bias unit which always outputs +1). The training data was loaded into the variables `X` and `y` above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  =  # 20x20 Input Images of Digits\n",
    "hidden_layer_size =  # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9\n",
    "\n",
    "# Initialise the random weights into variables w1 and w2\n",
    "\n",
    "\n",
    "# w1 has size \n",
    "# w2 has size \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Unroll parameters \n",
    "\n",
    "#print(nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j --->j+1 sj, s(j+1); s(j+1)*s(j)+1\n",
    "w1.shape\n",
    "w2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feedforward Propagation and Prediction\n",
    "\n",
    "Now you will implement feedforward propagation for the neural network. You will need to complete the code in the function\n",
    "`predict` to return the neural network’s prediction. You should implement the feedforward computation that computes\n",
    "$h_\\theta(x^{(i)})$ for every example $i$ and returns the associated predictions. Similar to the one-vs-all classification\n",
    "strategy, the prediction from the neural network will be the label that has the largest output $\\left( h_\\theta(x) \\right)_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w1, w2, X):\n",
    "    \"\"\"\n",
    "    Predict the label of an input given a trained neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w1 : array_like\n",
    "        Weights for the first layer in the neural network.\n",
    "        It has shape (2nd hidden layer size x input size)\n",
    "    \n",
    "    w2: array_like\n",
    "        Weights for the second layer in the neural network. \n",
    "        It has shape (output layer size x 2nd hidden layer size)\n",
    "    \n",
    "    X : array_like\n",
    "        The image inputs having shape (number of examples x image dimensions).\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    p : array_like\n",
    "        Predictions vector containing the predicted label for each example.\n",
    "        It has a length equal to the number of examples.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    if X.ndim==1:\n",
    "        X=X[None]\n",
    "    \n",
    "    m=X.shape[0]\n",
    "    num_labels= w2.shape[0]\n",
    "    \n",
    "    p= np.zeros(m)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Method\n",
    "\n",
    "### Unregularized Cost Function\n",
    "\n",
    "Now you will implement the cost function and gradient for the neural network.\n",
    "\n",
    "Recall that the cost function for the neural network (without regularization) is:\n",
    "\n",
    "$$ J(w) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_w \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_w \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "where $h_w \\left( x^{(i)} \\right)$ is computed as shown in the neural network figure above, and K = 10 is the total number of possible labels. Note that $h_w(x^{(i)})_k = a_k^{(3)}$ is the activation (output\n",
    "value) of the $k^{th}$ output unit. Also, recall that whereas the original labels (in the variable y) were 0, 1, ..., 9, for the purpose of training a neural network, we need to encode the labels as vectors containing only values 0 or 1, so that\n",
    "\n",
    "$$ y = \n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\\\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\cdots  \\quad \\text{or} \\qquad\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ (that you should use with the cost function) should be a 10-dimensional vector with $y_5 = 1$, and the other elements equal to 0.\n",
    "\n",
    "You should implement the feedforward computation that computes $h_w(x^{(i)})$ for every example $i$ and sum the cost over all examples. **Your code should also work for a dataset of any size, with any number of labels** (you can assume that there are always at least $K \\ge 3$ labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized cost function\n",
    "\n",
    "The cost function for neural networks with regularization is given by:\n",
    "\n",
    "\n",
    "$$ J(w) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_w \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_w \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{l=1}^{L-1} \\sum_{i=1}^{S_{l}} \\sum_{j=1}^{S_{l+1}} \\left( w_{ji}^{(l)} \\right)^2 \\right] $$\n",
    "\n",
    "Note that the terms corresponding to the bias should not be regularized. For the matrices `w1` and `w2`, this corresponds to the first column of each matrix. You should now add regularization to your cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  =   # 20x20 Input Images of Digits\n",
    "hidden_layer_size =   # 25 hidden units\n",
    "num_labels =    10   # 10 labels, from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function for a two layer neural \n",
    "    network which performs classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector.\n",
    "    \n",
    "    input_layer_size : int : Number of features for the input layer. \n",
    "    \n",
    "    hidden_layer_size : int : Number of hidden units in the second layer.\n",
    "    \n",
    "    num_labels : int : Total number of labels, or equivalently number of units in output layer. \n",
    "    \n",
    "    X : array_like : Input dataset. A matrix of shape (m x input_layer_size).\n",
    "    \n",
    "    y : array_like : Dataset labels. A vector of shape (m,).\n",
    "    \n",
    "    lambda_ : float, optional : Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float : The computed value for the cost function at the current weight values.\n",
    "    \n",
    "    \"\"\"\n",
    "    # nn_params----> 25*401+10*26 unrolled into a row vector; s(j+1)*s(j)+1\n",
    "    # Reshape nn_params back into the parameters w1 and w2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    w1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    w2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    w1_grad = np.zeros(w1.shape)\n",
    "    w2_grad = np.zeros(w2.shape)\n",
    "\n",
    "    # ================================================================================================\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # ===================================================================================================\n",
    "  \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation- A brief introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is the heart of every neural network. Firstly, we need to make a distinction between backpropagation and optimizers.\n",
    "\n",
    "Backpropagation is for calculating the gradients efficiently, while optimizers is for training the neural network, using the gradients computed with backpropagation. In short, all backpropagation does for us is compute the gradients. Nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always start from the output layer and propagate backwards, updating weights and biases for each layer.\n",
    "\n",
    "The idea is simple: adjust the weights and biases throughout the network, so that we get the desired output in the output layer. Say we wanted the output neuron to be 1.0, then we would need to nudge the weights and biases so that we get an output closer to 1.0.\n",
    "\n",
    "We can only change the weights and biases, but activations are direct calculations of those weights and biases, which means we indirectly can adjust every part of the neural network, to get the desired output — except for the input layer, since that is the dataset that you input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's start by defining the relevant equations. Note that any indexing explained earlier is left out here, and we abstract to each layer instead of each weight, bias or activation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "z^{(L)}=w^{(L)} \\times x +b \\newline\n",
    "a^{(L)}=\n",
    "    g\\left(\n",
    "    \\boldsymbol{z}^{(L)}\n",
    "    \\right) \\newline\n",
    "    C=(a^{(L)}-y)^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **gradient** of this Cost Function C in the backpropagation algorithm, can be then be thought of as the change  in the cost function in relation to a specific weight. Therefore the change of Cost Function with respect to weights w can be best captured by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w^{(L)}}\n",
    "    =\n",
    "    \\frac{\\partial C}{\\partial a^{(L)}}\n",
    "    \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "    \\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\n",
    "    =\n",
    "    2 \\left(a^{(L)} - y \\right) \\sigma' \\left(z^{(L)}\\right) x^{(L-1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you would update the weights after each mini-batch. Each weight is 'nudged' a certain amount for each layer l\n",
    "\\begin{equation}\n",
    "w^{(l)} = w^{(l)} - \\text{learning rate} \\times \\frac{\\partial C}{\\partial w^{(l)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to move all the way back through the network and adjust each weight, iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backpropagation Method\n",
    "\n",
    "In this part of the exercise, you will implement the backpropagation algorithm to compute the gradient for the neural network cost function. You will need to update the function `nnCostFunction` so that it returns an appropriate value for `grad`. Once you have computed the gradient, you will be able to train the neural network by minimizing the cost function $J(w)$ using an advanced optimizer such as `scipy`'s `optimize.minimize`.\n",
    "You will first implement the backpropagation algorithm to compute the gradients for the parameters for the (unregularized) neural network. After you have verified that your gradient computation for the unregularized case is correct, you will implement the gradient for the regularized neural network.\n",
    "\n",
    "<a id=\"section4\"></a>\n",
    "### Backpropagation\n",
    "\n",
    "![](Figures/ex4-backpropagation.png)\n",
    "\n",
    "Now, you will implement the backpropagation algorithm. Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example $(x^{(t)}, y^{(t)})$, we will first run a “forward pass” to compute all the activations throughout the network, including the output value of the hypothesis $h_w(x)$. Then, for each node $j$ in layer $l$, we would like to compute an “error term” $\\delta_j^{(l)}$ that measures how much that node was “responsible” for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the network’s activation and the true target value, and use that to define $\\delta_j^{(3)}$ (since layer 3 is the output layer). For the hidden units, you will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$. In detail, here is the backpropagation algorithm (also depicted in the figure above). You should implement steps 1 to 4 in a loop that processes one example at a time. Concretely, you should implement a for-loop `for t in range(m)` and place steps 1-4 below inside the for-loop, with the $t^{th}$ iteration performing the calculation on the $t^{th}$ training example $(x^{(t)}, y^{(t)})$. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "1. Set the input layer’s values $(a^{(1)})$ to the $t^{th }$training example $x^{(t)}$. Perform a feedforward pass, computing the activations $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ for layers 2 and 3. Note that you need to add a `+1` term to ensure that the vectors of activations for layers $a^{(1)}$ and $a^{(2)}$ also include the bias unit. In `numpy`, if a 1 is a column matrix, adding one corresponds to `a_1 = np.concatenate([np.ones((m, 1)), a_1], axis=1)`.\n",
    "\n",
    "1. For each output unit $k$ in layer 3 (the output layer), set \n",
    "$$\\delta_k^{(3)} = \\left(a_k^{(3)} - y_k \\right)$$\n",
    "where $y_k \\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k$ $(y_k = 1)$, or if it belongs to a different class $(y_k = 0)$. You may find logical arrays helpful for this task (explained in the previous programming exercise).\n",
    "\n",
    "1. For the hidden layer $l = 2$, set \n",
    "$$ \\delta^{(2)} = \\left( w^{(2)} \\right)^T \\delta^{(3)} * g'\\left(z^{(2)} \\right)$$\n",
    "Note that the symbol $*$ performs element wise multiplication in `numpy`.\n",
    "\n",
    "1. Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_0^{(2)}$. In `numpy`, removing $\\delta_0^{(2)}$ corresponds to `delta_2 = delta_2[1:]`.\n",
    "\n",
    "1. Obtain the (unregularized) gradient for the neural network cost function by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "$$ \\frac{\\partial}{\\partial w_{ij}^{(l)}} J(w) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "### Sigmoid Gradient\n",
    "\n",
    "To help you get started with this part of the exercise, you will first implement\n",
    "the sigmoid gradient function. The gradient for the sigmoid function can be\n",
    "computed as\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz} g(z) = g(z)\\left(1-g(z)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\text{sigmoid}(z) = g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Now complete the implementation of `sigmoidGradient` in the next cell.\n",
    "<a id=\"sigmoidGradient\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like : A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like : Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $w^{(l)}$ uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$. We are going to use $\\epsilon_{init} = 0.12$. This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "\n",
    "Use the function `randInitializeWeights` to initialize the weights for $w$. Note that the function has been provided with an argument for $\\epsilon$ with default value `epsilon_init = 0.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int : Number of incomming connections.\n",
    "    \n",
    "    L_out : int : Number of outgoing connections. \n",
    "    \n",
    "    epsilon_init : float, optional : Range of values which the weight can take from a uniform distribution.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialise weights for two different layers\n",
    "initial_w1 = \n",
    "initial_w2 = \n",
    "\n",
    "# Unroll parameters into a single array\n",
    "initial_nn_params = np.concatenate([initial_w1.ravel(), initial_w2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10041601,  0.0950005 ,  0.08269092, ..., -0.02902873,\n",
       "        -0.00144294, -0.00265943],\n",
       "       [ 0.005929  , -0.03991494, -0.11808299, ..., -0.11905432,\n",
       "        -0.04722452,  0.07686187],\n",
       "       [ 0.02313064, -0.04171044,  0.11470125, ..., -0.00589284,\n",
       "        -0.06976737,  0.0645766 ],\n",
       "       ...,\n",
       "       [ 0.00166044,  0.00501722, -0.08476042, ..., -0.05826802,\n",
       "         0.11582343, -0.0917649 ],\n",
       "       [-0.06753614, -0.07353709, -0.05513467, ...,  0.0667047 ,\n",
       "        -0.07267055, -0.06783383],\n",
       "       [-0.03155047, -0.01837105,  0.08431225, ...,  0.11692303,\n",
       "         0.04764311, -0.10000005]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "### Regularized Neural Network\n",
    "\n",
    "After you have successfully implemented the backpropagation algorithm, you also need to add regularization to the gradient. To account for regularization, it turns out that you can add this as an additional term *after* computing the gradients using backpropagation.\n",
    "\n",
    "Specifically, after you have computed $\\Delta_{ij}^{(l)}$ using backpropagation, you should add regularization using\n",
    "\n",
    "$$ \\begin{align} \n",
    "& \\frac{\\partial}{\\partial w_{ij}^{(l)}} J(w) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial}{\\partial w_{ij}^{(l)}} J(w) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\frac{\\lambda}{m} w_{ij}^{(l)} & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that you should *not* be regularizing the first column of $w^{(l)}$ which is used for the bias term. Furthermore, in the parameters $w_{ij}^{(l)}$, $i$ is indexed starting from 1, and $j$ is indexed starting from 0. Thus, \n",
    "\n",
    "$$\n",
    "w^{(l)} = \\begin{bmatrix}\n",
    "w_{1,0}^{(i)} & w_{1,1}^{(l)} & \\cdots \\\\\n",
    "w_{2,0}^{(i)} & w_{2,1}^{(l)} & \\cdots \\\\\n",
    "\\vdots &  ~ & \\ddots\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function and gradient for a two layer neural \n",
    "    network which performs classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector.\n",
    "    \n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer. \n",
    "    \n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer.\n",
    "    \n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer. \n",
    "    \n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size).\n",
    "    \n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float : The computed value for the cost function at the current weight values.\n",
    "    \n",
    "    grad : array_like : An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "                        neural network weights w1 and w2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape nn_params back into the parameters w1 and w2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    w1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    w2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    w1_grad = np.zeros(w1.shape)\n",
    "    w2_grad = np.zeros(w2.shape)\n",
    "\n",
    "    # ================================================================================================\n",
    "    \n",
    "\n",
    "    \n",
    "    # Add regularization term\n",
    "    \n",
    "\n",
    "    # Backpropogation\n",
    "\n",
    "    \n",
    "    # Add regularization to gradient\n",
    "\n",
    "    \n",
    "\n",
    "    #=======================================================================================================\n",
    "   \n",
    "    return J,grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters using `scipy.optimize.minimize`\n",
    "\n",
    "After you have successfully implemented the neural network cost function\n",
    "and gradient computation, the next step we will use `scipy`'s minimization to learn a good set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger value to see how more training helps.\n",
    "options= {'maxiter': 100}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = \n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size, # p == nn_param\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.6933524165748763\n",
       "     jac: array([ 1.68322852e-03, -2.04567051e-05,  2.72947999e-05, ...,\n",
       "       -3.93194162e-03,  1.14572119e-04, -2.89250002e-03])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 101\n",
       "     nit: 12\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([ 0.45554158, -0.03409451,  0.04549133, ...,  1.15501903,\n",
       "       -0.29830072,  1.07079938])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the solution of the optimization\n",
    "nn_params = res.x #Row vector containing weight values for every layer of your implemented neural network\n",
    "        \n",
    "# Obtain w1 and w2 back from nn_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training completes, we will proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct. If your implementation is correct, you should see a reported\n",
    "training accuracy of about 95.3% (this may vary by about 1% due to the random initialization). It is possible to get higher training accuracies by training the neural network for more iterations. We encourage you to try\n",
    "training the neural network for more iterations (e.g., set `maxiter` to 400) and also vary the regularization parameter $\\lambda$. With the right learning settings, it is possible to get the neural network to perfectly fit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 94.760000\n"
     ]
    }
   ],
   "source": [
    "pred = predict(w1, w2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if indices.size > 0:\n",
    "    i, indices = indices[0], indices[1:]\n",
    "    displayData(X[i, :], figsize=(4, 4))\n",
    "    pred = predict(w1, w2, X[i, :])\n",
    "    print('Neural Network Prediction: {}'.format(*pred))\n",
    "else:\n",
    "    print('No more images to display!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary: One Hot Vector Encoding\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/31fk7i/converting_target_indices_to_onehotvector/\n",
    "https://stackoverflow.com/questions/45068853/how-does-this-one-hot-vector-conversion-work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.minimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
