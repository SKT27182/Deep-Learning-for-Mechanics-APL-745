{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:\n",
    "Derive systematically the steps involving the backpropagation of a neural network\n",
    "considered in the class with two hidden layers using the cross-entropy loss function. The activation\n",
    "function for each layer can be considered to be **ReLU**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notations**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scalars are denoted by lowercase letters (e.g. $x$)\n",
    "* Vectors are denoted by bold lowercase letters (e.g. $\\mathbf{x}$), `bold`\n",
    "* Matrices are denoted by bold uppercase letters (e.g. $\\mathbf{X}$), `bold uppercase`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_{i}$ denotes the $i^{th}$ element of the vector $\\mathbf{x}$\n",
    "* $x_{ij}$ denotes the element in the $i^{th}$ row and $j^{th}$ column of the matrix $\\mathbf{X}$\n",
    "* $\\mathbf{x}^{[i]}$ denotes the $i^{th}$ coloumn of the matrix $\\mathbf{X}$\n",
    "* $\\mathbf{x}_{j}$ denotes the $j^{th}$ row of the matrix "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $n^{[l]}$ denotes the number of neurons in the $l^{th}$ layer\n",
    "\n",
    "* $n_{x}$ denotes the number of features in the input layer\n",
    "\n",
    "* $n_{y}$ denotes the number of nuerons in the output layer which is equal to the number of classes for classification problems\n",
    "\n",
    "* $m$ denotes the number of training examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapes\n",
    "\n",
    "* $\\mathbf{X} \\in \\mathbb{R}^{n_x \\times m}$, is a input matrix.\n",
    "\n",
    "* $\\mathbf{\\hat{Y}} \\in \\mathbb{R}^{n_y \\times m}$, is a predicted output matrix. \n",
    "\n",
    "$$\\hat{Y} = \\mathbf{W}^{[L]}\\mathbf{A}^{[L-1]} + \\mathbf{b}^{[L]}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mathbf{Y} \\in \\mathbb{R}^{n_y \\times m}$, is a output matrix.\n",
    "\n",
    "* $\\mathbf{W}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$, is a weight matrix.\n",
    "\n",
    "* $\\mathbf{b}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times 1}$, is a bias vector.\n",
    "\n",
    "* $\\mathbf{A}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$, is a output matrix.\n",
    "\n",
    "* $\\mathbf{Z}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$, is a matrix of linear activations.\n",
    "\n",
    "$$\\mathbf{Z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{A}^{[l-1]} + \\mathbf{b}^{[l]}$$\n",
    "\n",
    "$$\\mathbf{A}^{[0]} = \\mathbf{X} \\text{,   which is input}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* $g^{[l]}$, is the activation function of the $l^{th}$ layer.\n",
    "\n",
    "* $\\mathbf{A}^{[l]} = g^{[l]}(\\mathbf{Z}^{[l]})$, is a matrix of activations where $g^{[l]}$ is the activation function of the $l^{th}$ layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "* We have input of shape $(n_{x}, m)$\n",
    "\n",
    "- **First hidden layer** : we have $n^{[1]}$ nuerons. We initialize the weights and bias as follows:\n",
    "$$\\mathbf{W}^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times n_{x}}$$\n",
    "$$\\mathbf{b}^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times 1}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Second hidden layer** : we have $n^{[2]} neurons$. We initialize the weights and bias as follows:\n",
    "$$\\mathbf{W}^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times n^{[1]}}$$\n",
    "$$\\mathbf{b}^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times 1}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Output layer** : we have $n^{[3]}$ or $n_{y}$ nuerons. We initialize the weights and bias as follows:\n",
    "$$\\mathbf{W}^{[3]} \\in \\mathbb{R}^{n^{[3]} \\times n^{[2]}}$$\n",
    "$$\\mathbf{b}^{[3]} \\in \\mathbb{R}^{n^{[3]} \\times 1}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "1. At the `first layer`, we have $A^{[0]} = \\mathbf{X}$, which is the input.\n",
    "\n",
    "- we will calculate the linear activation $\\mathbf{Z}^{[1]}$ and the activation $\\mathbf{A}^{[1]}$ as follows:\n",
    "$$\\mathbf{Z}^{[1]} = \\mathbf{W}^{[1]}\\mathbf{A}^{[0]} + \\mathbf{b}^{[1]}$$\n",
    "\n",
    "$$\\mathbf{A}^{[1]} = g^{[1]}(\\mathbf{Z}^{[1]})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. At the `second layer`, we have $\\mathbf{A}^{[1]}$ as the input.\n",
    "\n",
    "- we will calculate the linear activation $\\mathbf{Z}^{[2]}$ and the activation $\\mathbf{A}^{[2]}$ as follows:\n",
    "$$\\mathbf{Z}^{[2]} = \\mathbf{W}^{[2]}A^{[1]} + \\mathbf{b}^{[2]}$$\n",
    "\n",
    "$$\\mathbf{A}^{[2]} = g^{[2]}(\\mathbf{Z}^{[2]})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now at the `output layer`, we have $\\mathbf{A}^{[3]}$ as the input.\n",
    "\n",
    "- we will calculate the linear activation $\\mathbf{Z}^{[3]}$ and the activation $\\mathbf{A}^{[3]}$ as follows:\n",
    "$$\\mathbf{Z}^{[3]} = \\mathbf{W}^{[3]}\\mathbf{A}^{[2]} + \\mathbf{b}^{[3]}$$\n",
    "\n",
    "$$\\mathbf{A}^{[3]} = g^{[3]}(\\mathbf{Z}^{[3]})$$\n",
    "\n",
    "$g^{[3]}$ is the activation function of the output layer. For classification problems, we use the softmax function. For regression problems, we use the identity function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\mathbf{A}^{[3]}$ is the output of the neural network.\n",
    "\n",
    "It can be seen as the function of it's previous layers parameters and the input $\\mathbf{X}$.\n",
    "\n",
    "$$\\mathbf{A}^{[3]} = f(\\mathbf{X}, \\mathbf{W}^{[1]}, \\mathbf{b}^{[1]}, \\mathbf{W}^{[2]}, \\mathbf{b}^{[2]}, \\mathbf{W}^{[3]}, \\mathbf{b}^{[3]})$$\n",
    "\n",
    "$$ \\mathbf{Y} = f(g^{[3]}(g^{[2]}(g^{[1]}(\\mathbf{X}, \\mathbf{W}^{[1]}, \\mathbf{b}^{[1]}), \\mathbf{W}^{[2]}, \\mathbf{b}^{[2]}), \\mathbf{W}^{[3]}, \\mathbf{b}^{[3]}))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation\n",
    "\n",
    "* Now we have to update the parameters of the neural network to minimize the loss function.\n",
    "\n",
    "* We will use the `cross-entropy loss` function.\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_{k}^{(i)} \\log(\\hat{y}_{k}^{(i)})$$\n",
    "\n",
    "this is a scalar value.\n",
    "\n",
    "here shape of $\\mathbf{Y}$ is $(n_{y}, m)$ and shape of $\\mathbf{\\hat{Y}}$ is $(n_{y}, m)$\n",
    "\n",
    "so $y_{k}^{(i)}$ is the $k^{th}$ element of the $i^{th}$  sample of $\\mathbf{Y}$ \n",
    "\n",
    "and $\\hat{y}_{k}^{(i)}$ is the $k^{th}$ element of the $i^{th}$  sample of $\\mathbf{\\hat{Y}}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* We will use the gradient descent algorithm to update the parameters. \n",
    "\n",
    "* We will use the chain rule to calculate the gradients of the loss function with respect to the parameters. \n",
    "\n",
    "* As Loss `J` is a function of the parameters `W` and `b`, we can calculate the gradients of `J` with respect to `W` and `b` as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{W}^{[l]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[l]}} \\frac{\\partial \\mathbf{A}^{[l]}}{\\partial \\mathbf{Z}^{[l]}} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{W}^{[l]}}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b}^{[l]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[l]}} \\frac{\\partial \\mathbf{A}^{[l]}}{\\partial \\mathbf{Z}^{[l]}} \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{b}^{[l]}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "\n",
    "* We will calculate the gradient of the loss function `(categorical-cross entropy)` with respect to the $A^{[3]}$ as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{A}^{[3]}} = -\\frac{1}{m}(\\frac{\\mathbf{Y}}{\\mathbf{\\hat{Y}}})$$\n",
    "\n",
    "* Now we will calculate the gradients of the $\\mathbf{A}^{[3]}$ `(softmax)` with respect to the $\\mathbf{Z}^{[3]}$ as follows:\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{A_{i}}^{[3]}}{\\partial \\mathbf{Z_{j}}^{[3]}} = \\mathbf{A_{i}}^{[3]}(\\delta_{ij} - \\mathbf{A_{i}}^{[3]})$$\n",
    "\n",
    "As we are using the `softmax activation` function at the output layer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we will calculate the gradients of the $\\mathbf{Z}^{[3]}$ with respect to the $\\mathbf{W}^{[3]}$ and $\\mathbf{b}^{[3]}$ as follows:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{Z}^{[3]}}{\\partial \\mathbf{W}^{[3]}} = \\mathbf{A}^{[2]}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{Z}^{[3]}}{\\partial \\mathbf{b}^{[3]}} = 1$$\n",
    "\n",
    "hence we have the following equations:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b}^{[3]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[3]}} \\frac{\\partial \\mathbf{A}^{[3]}}{\\partial \\mathbf{Z}^{[3]}} \\frac{\\partial \\mathbf{Z}^{[3]}}{\\partial \\mathbf{b}^{[3]}} $$\n",
    "\n",
    "$$ =( -\\frac{1}{m}(\\frac{\\mathbf{Y}}{\\mathbf{\\hat{Y}}}) ) * (\\mathbf{A_{i}}^{[3]}(\\delta_{ij} - \\mathbf{A_{i}}^{[3]}) ) * ( 1 ) = -\\frac{1}{m}( \\mathbf{\\hat{Y}} - \\mathbf{Y} ) * (1) = \\delta^{[3]}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{W}^{[3]}} = \\delta^{[3]} * ( \\mathbf{A}^{[2]} )$$\n",
    "\n",
    "where $\\delta^{[3]}$ is the error of the output layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to update the weights and bias of the output layer, we will use the following equations:\n",
    "\n",
    "$$\\mathbf{W}^{[3]} = \\mathbf{W}^{[3]} - \\alpha \\frac{\\partial J}{\\partial W^{[3]}}$$\n",
    "\n",
    "$$\\mathbf{b}^{[3]} = \\mathbf{b}^{[3]} - \\alpha \\frac{\\partial J}{\\partial b^{[3]}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "Here we will pass the $\\delta^{[3]}$ to the previous layer by multiplying it with the weights of this layer.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{A}^{[2]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[3]}} \\frac{\\partial \\mathbf{A}^{[3]}}{\\partial \\mathbf{Z}^{[3]}} \\frac{\\partial \\mathbf{Z}^{[3]}}{\\partial \\mathbf{A}^{[2]}}$$\n",
    "\n",
    "$$ = \\delta^{[3]} * ( \\mathbf{W}^{[3]} )$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second hidden layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will update the weights and bias of the second hidden layer. i.e. $\\mathbf{W}^{[2]}$ and $\\mathbf{b}^{[2]}$.\n",
    "\n",
    "For that we need to calculate the gradients of the loss function with respect to the $\\mathbf{W}^{[2]}$ and $\\mathbf{b}^{[2]}$.\n",
    "\n",
    "So, we will calculate the gradients of the $\\mathbf{A}^{[2]}$ with respect to the $\\mathbf{Z}^{[2]}$ as follows:\n",
    "as activation function at the second hidden layer is relu, so we will use the following equation:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{A}^{[2]}}{\\partial \\mathbf{Z}^{[2]}} = \\begin{cases} 1 & \\text{if } \\mathbf{Z}^{[2]} > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the gradients of the $\\mathbf{Z}^{[2]}$ with respect to the $\\mathbf{W}^{[2]}$ and $\\mathbf{b}^{[2]}$ as follows:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{W}^{[2]}} = \\mathbf{A}^{[1]}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{b}^{[2]}} = 1$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the following equations:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b}^{[2]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[3]}} \\frac{\\partial \\mathbf{A}^{[3]}}{\\partial \\mathbf{Z}^{[3]}} \\frac{\\partial \\mathbf{Z}^{[3]}}{\\partial \\mathbf{A}^{[2]}} \\frac{\\partial \\mathbf{A}^{[2]}}{\\partial \\mathbf{Z}^{[2]}} \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{b}^{[2]}}$$\n",
    "\n",
    "$$ = \\delta^{[3]} * ( \\mathbf{W}^{[3]} ) * ( \\begin{cases} 1 & \\text{if } \\mathbf{Z}^{[2]} > 0 \\\\ 0 & \\text{otherwise} \\end{cases} ) * ( 1 ) = \\delta^{[2]}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{W}^{[2]}} = \\delta^{[2]} * (\\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{W}^{[2]}})$$\n",
    "\n",
    "$$ = \\delta^{[2]} * (\\mathbf{A}^{[1]})$$\n",
    "\n",
    "where $\\delta^{[2]}$ is the error of the second hidden layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to update the weights and bias of the second hidden layer, we will use the following equations:\n",
    "\n",
    "$$\\mathbf{W}^{[2]} = \\mathbf{W}^{[2]} - \\alpha \\frac{\\partial J}{\\partial W^{[2]}}$$\n",
    "\n",
    "$$\\mathbf{b}^{[2]} = \\mathbf{b}^{[2]} - \\alpha \\frac{\\partial J}{\\partial b^{[2]}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "Here we will pass the $\\delta^{[2]}$ to the previous layer by multiplying it with the weights of this layer.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{A}^{[1]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[3]}} \\frac{\\partial \\mathbf{A}^{[3]}}{\\partial \\mathbf{Z}^{[3]}} \\frac{\\partial \\mathbf{Z}^{[3]}}{\\partial \\mathbf{A}^{[2]}} \\frac{\\partial \\mathbf{A}^{[2]}}{\\partial \\mathbf{Z}^{[2]}} \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{A}^{[1]}}$$\n",
    "\n",
    "$$ = \\delta^{[2]} * ( \\mathbf{W}^{[2]} )$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First hidden layer\n",
    "\n",
    "Here we will update the weights and bias of the first hidden layer. i.e. $\\mathbf{W}^{[1]}$ and $\\mathbf{b}^{[1]}$.\n",
    "\n",
    "For that we need to calculate the gradients of the loss function with respect to the $\\mathbf{W}^{[1]}$ and $\\mathbf{b}^{[1]}$.\n",
    "\n",
    "So, we will calculate the gradients of the $\\mathbf{A}^{[1]}$ with respect to the $\\mathbf{Z}^{[1]}$ as follows:\n",
    "\n",
    "as activation function at the first hidden layer is relu, so we will use the following equation:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{A}^{[1]}}{\\partial \\mathbf{Z}^{[1]}} = \\begin{cases} 1 & \\text{if } \\mathbf{Z}^{[1]} > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the gradients of the $\\mathbf{Z}^{[1]}$ with respect to the $\\mathbf{W}^{[1]}$ and $\\mathbf{b}^{[1]}$ as follows:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{Z}^{[1]}}{\\partial \\mathbf{W}^{[1]}} = \\mathbf{A}^{[0]} = \\mathbf{X}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{Z}^{[1]}}{\\partial \\mathbf{b}^{[1]}} = 1$$\n",
    "\n",
    "So we have the following equations:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b}^{[1]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[3]}} \\frac{\\partial \\mathbf{A}^{[3]}}{\\partial \\mathbf{Z}^{[3]}} \\frac{\\partial \\mathbf{Z}^{[3]}}{\\partial \\mathbf{A}^{[2]}} \\frac{\\partial \\mathbf{A}^{[2]}}{\\partial \\mathbf{Z}^{[2]}} \\frac{\\partial \\mathbf{Z}^{[2]}}{\\partial \\mathbf{A}^{[1]}} \\frac{\\partial \\mathbf{A}^{[1]}}{\\partial \\mathbf{Z}^{[1]}} \\frac{\\partial \\mathbf{Z}^{[1]}}{\\partial \\mathbf{b}^{[1]}}$$\n",
    "\n",
    "$$ = \\delta^{[2]} * ( \\mathbf{W}^{[2]} ) * ( \\begin{cases} 1 & \\text{if } \\mathbf{Z}^{[1]} > 0 \\\\ 0 & \\text{otherwise} \\end{cases} ) * ( 1 ) = \\delta^{[1]}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{W}^{[1]}} = \\delta^{[1]} * (\\frac{\\partial \\mathbf{Z}^{[1]}}{\\partial \\mathbf{W}^{[1]}})$$\n",
    "\n",
    "$$ = \\delta^{[1]} * (\\mathbf{A}^{[0]}) = \\delta^{[1]} * (\\mathbf{X})$$\n",
    "\n",
    "where $\\delta^{[1]}$ is the error of the first hidden layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to update the weights and bias of the first hidden layer, we will use the following equations:\n",
    "\n",
    "$$\\mathbf{W}^{[1]} = \\mathbf{W}^{[1]} - \\alpha \\frac{\\partial J}{\\partial W^{[1]}}$$\n",
    "\n",
    "$$\\mathbf{b}^{[1]} = \\mathbf{b}^{[1]} - \\alpha \\frac{\\partial J}{\\partial b^{[1]}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So we have now updated the weights and bias of all the layers.\n",
    "\n",
    "Now we will repeat the above steps for all the training examples and then update the weights and bias of all the layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization \n",
    "\n",
    "1. Forward propagation\n",
    "\n",
    "$$ \\mathbf{A}^{[0]} = \\mathbf{X} $$\n",
    "\n",
    "$$\\mathbf{Z}^{[l]} = \\mathbf{W}^{[l]} \\mathbf{A}^{[l-1]} + \\mathbf{b}^{[l]}$$\n",
    "\n",
    "$$\\mathbf{A}^{[l]} = g^{[l]}(\\mathbf{Z}^{[l]})$$\n",
    "\n",
    "where $g^{[l]}$ is the activation function of the $l^{th}$ layer. and l=1,2,3,4...\n",
    "\n",
    "2. Backward propagation\n",
    "\n",
    "* We have Loss function as Cross entropy loss function. $$J = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_{k}^{(i)} \\log(\\hat{y}_{k}^{(i)})$$\n",
    "\n",
    "So gradient of Loss function with respect to $\\mathbf{A}^{[L]}$ is:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{A}^{[L]}} = -\\frac{1}{m} \\sum_{i=1}^{m} \\frac{y^{(i)}}{\\hat{y}^{(i)}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* General updating rule for the weights and bias of the $l^{th}$ layer is:\n",
    "\n",
    "$$\\mathbf{W}^{[l]} = \\mathbf{W}^{[l]} - \\alpha \\frac{\\partial J}{\\partial W^{[l]}}$$\n",
    "\n",
    "$$\\mathbf{b}^{[l]} = \\mathbf{b}^{[l]} - \\alpha \\frac{\\partial J}{\\partial b^{[l]}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here $\\frac{\\partial J}{\\partial W^{[l]}}$ is the gradient of the loss function with respect to the weights of the $l^{th}$ layer.\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial W^{[l]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[L]}} \\frac{\\partial \\mathbf{A}^{[L]}}{\\partial \\mathbf{Z}^{[L]}} \\frac{\\partial \\mathbf{Z}^{[L]}}{\\partial \\mathbf{A}^{[L-1]}} ... \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{W}^{[l]}} $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b^{[l]}} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[L]}} \\frac{\\partial \\mathbf{A}^{[L]}}{\\partial \\mathbf{Z}^{[L]}} \\frac{\\partial \\mathbf{Z}^{[L]}}{\\partial \\mathbf{A}^{[L-1]}} ... \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{b}^{[l]}} $$\n",
    "$$ \\frac{\\partial J}{\\partial \\mathbf{A}^{[L]}} = -\\frac{1}{m} \\sum_{i=1}^{m} \\frac{y^{(i)}}{\\hat{y}^{(i)}}$$\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{A}^{[l]}}{\\partial \\mathbf{Z}^{[l]}} = g^{[l]'}(\\mathbf{Z}^{[l]}) \\text{ } and \\text{ } \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{A}^{[l-1]}} = \\mathbf{W}^{[l]}$$\n",
    "\n",
    "\n",
    "where $\\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{W}^{[l]}} = \\mathbf{A}^{[l-1]}$ and $\\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{b}^{[l]}} = 1$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can find the $\\frac{\\partial J}{\\partial \\mathbf{W}^{[l]}}$ and $\\frac{\\partial J}{\\partial \\mathbf{b}^{[l]}}$ for any layer $l$ and then update the weights and bias in the backpropagation step. and then repeat this again and again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4a1b8dfbfcf197657bbe13300384e076b1914ebd98f1200feef09f71d03b223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
